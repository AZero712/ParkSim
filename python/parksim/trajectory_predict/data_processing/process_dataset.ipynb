{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from dlp.dataset import Dataset\n",
    "from dlp.visualizer import MAP_SIZE, Visualizer\n",
    "from math import cos, sin\n",
    "from parksim.trajectory_predict.data_processing.utils import TransformerDataProcessor\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "import pickle as pkl\n",
    "from vehicle import Vehicle, Location, Size3d, get_box_pts_from_center_heading\n",
    "from math import pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vehicle对象存储跟输入输出相关的所有的东西"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = 10\n",
    "history = 5\n",
    "future = 5\n",
    "cpu_count = os.cpu_count()\n",
    "ds = Dataset()\n",
    "ds.load(f\"{Path.home()}/dlp-dataset/data/\"+ \"DJI_0012\")\n",
    "vis = Visualizer(ds)\n",
    "scene_token = ds.list_scenes()[0]\n",
    "scene = ds.get('scene', scene_token)\n",
    "agents_token = scene['agents']\n",
    "agent_token2idx = {agent_token: str(idx) for idx, agent_token in enumerate(agents_token)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 获取局部地图四个角点在全局坐标系的位置\n",
    "- 筛选出在局部地图内的agent\n",
    "- 筛选障碍物\n",
    "- 筛选空和占用车库\n",
    "- 目标点 车库或道路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_overlap(rect1, rect2):\n",
    "    # 计算每个矩形的最小和最大x和y坐标\n",
    "    min_x1, min_y1 = np.min(rect1, axis=0)\n",
    "    max_x1, max_y1 = np.max(rect1, axis=0)\n",
    "    min_x2, min_y2 = np.min(rect2, axis=0)\n",
    "    max_x2, max_y2 = np.max(rect2, axis=0)\n",
    "\n",
    "    # 检查两个矩形是否重叠\n",
    "    if min_x1 > max_x2 or min_x2 > max_x1 or min_y1 > max_y2 or min_y2 > max_y1:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_instance(inst_token: str, inst_idx: int, frame_token: str, extractor: TransformerDataProcessor, ds: Dataset, agent_token2idx: dict, vis: Visualizer):\n",
    "    veh = Vehicle()\n",
    "    curr_instance = ds.get('instance', inst_token)\n",
    "    agent_token = curr_instance['agent_token']\n",
    "    veh.id = agent_token2idx[agent_token]\n",
    "    veh.location = Location(curr_instance[\"coords\"][0], curr_instance[\"coords\"][1], None)\n",
    "    veh.speed_heading = curr_instance[\"heading\"] / pi * 180\n",
    "    veh.size = Size3d(1.8, 3.6, 1.5)\n",
    "    veh.safe_size = Size3d(2.0, 3.8, 1.5)\n",
    "    veh.update_poly_box_and_realworld_4_vertices()\n",
    "    veh.update_safe_poly_box()\n",
    "\n",
    "    curr_pose = np.array([curr_instance['coords'][0],\n",
    "                         curr_instance['coords'][1], curr_instance['heading']])\n",
    "    rot = np.array([[cos(-curr_pose[2]), -sin(-curr_pose[2])], [sin(-curr_pose[2]), cos(-curr_pose[2])]])\n",
    "    global_map_size = MAP_SIZE\n",
    "    # 以当前车的位置为坐标系，计算局部地图窗口4个顶点的坐标，窗口大小为inst_ctr_size\n",
    "    inst_ctr_size = 40\n",
    "    map_global_4_vertices = get_box_pts_from_center_heading(length=inst_ctr_size, width=inst_ctr_size, xc=veh.location.x, yc=veh.location.y, heading=veh.speed_heading)\n",
    "    map_global_4_vertices = np.array(map_global_4_vertices)\n",
    "    # 将局部地图超出全局地图的部分裁剪掉\n",
    "    map_global_4_vertices[:, 0] = np.clip(map_global_4_vertices[:, 0], 0, global_map_size['x'])\n",
    "    map_global_4_vertices[:, 1] = np.clip(map_global_4_vertices[:, 1], 0, global_map_size['y'])\n",
    "    # 将map_global_4_vertices转为以当前车为坐标系的坐标\n",
    "    map_local_4_vertices = map_global_4_vertices.copy()\n",
    "    map_local_4_vertices = np.dot(rot, (map_local_4_vertices - curr_pose[:2]).T).T\n",
    "    veh.map_local_4_vertices = map_local_4_vertices\n",
    "\n",
    "    curr_frame = ds.get('frame', frame_token)\n",
    "    all_curr_instance_tokens = curr_frame['instances']\n",
    "    selected_instance_tokens=[]\n",
    "    for temp_instance_token in all_curr_instance_tokens:\n",
    "        temp_instance = ds.get('instance', temp_instance_token)\n",
    "        # 判断当前instance是否在局部地图窗口内\n",
    "        # 转化为以当前车为坐标系的坐标\n",
    "        temp_instance_local_coord = np.dot(rot, (np.array(temp_instance['coords']) - curr_pose[:2]).T).T\n",
    "        if np.all(np.fabs(temp_instance_local_coord) < inst_ctr_size/2):\n",
    "            selected_instance_tokens.append(temp_instance_token)\n",
    "    for temp_instance_token in selected_instance_tokens:\n",
    "        # 获取车辆的历史轨迹\n",
    "        temp_bv_history = []\n",
    "        temp_instance = ds.get('instance', temp_instance_token)\n",
    "        for timestep in range(0, stride*history, stride):\n",
    "            temp_history_instance = ds.get_agent_past(temp_instance_token, timestep)[0]\n",
    "            temp_history_instance_local_coord = np.dot(rot, (np.array(temp_history_instance['coords']) - curr_pose[:2]).T).T\n",
    "\n",
    "            temp_history_instance_local_heading = temp_history_instance['heading'] - curr_pose[2]\n",
    "            temp_history_instance_local_pose = np.concatenate([temp_history_instance_local_coord, [temp_history_instance_local_heading]])\n",
    "            temp_bv_history.append(temp_history_instance_local_pose)\n",
    "        temp_bv_history.reverse()\n",
    "        temp_bv_history = np.array(temp_bv_history)\n",
    "        if agent_token == temp_instance['agent_token']:\n",
    "            veh.av_history = temp_bv_history\n",
    "        else:\n",
    "            veh.bvs_history_list.append(temp_bv_history)\n",
    "        # 获取车辆的未来轨迹\n",
    "        temp_bv_future = []\n",
    "        for timestep in range(stride, stride*future+1, stride):\n",
    "            temp_future_instance = ds.get_agent_future(temp_instance_token, timestep)[-1]\n",
    "            temp_future_instance_local_coord = np.dot(rot, (np.array(temp_future_instance['coords']) - curr_pose[:2]).T).T\n",
    "            temp_future_instance_local_heading = temp_future_instance['heading'] - curr_pose[2]\n",
    "            temp_future_instance_local_pose = np.concatenate([temp_future_instance_local_coord, [temp_future_instance_local_heading]])\n",
    "            temp_bv_future.append(temp_future_instance_local_pose)\n",
    "        temp_bv_future = np.array(temp_bv_future)\n",
    "        if agent_token == temp_instance['agent_token']:\n",
    "            veh.av_future = temp_bv_future\n",
    "        else:\n",
    "            veh.bvs_future_list.append(temp_bv_future)\n",
    "\n",
    "    # 获取局部地图窗口内的障碍物\n",
    "    scene_token = ds.list_scenes()[0]\n",
    "    scene = ds.get('scene', scene_token)\n",
    "    all_obstacle_tokens = scene['obstacles']\n",
    "    selected_obstacle_tokens=[]\n",
    "    for temp_obstacle_token in all_obstacle_tokens:\n",
    "        temp_obstacle = ds.get('obstacle', temp_obstacle_token)\n",
    "        # temp_obstacle_local_coord = np.dot(np.array(temp_obstacle['coords']) - curr_pose[:2], rot)\n",
    "        temp_obstacle_local_coord = np.dot(rot, (np.array(temp_obstacle['coords']) - curr_pose[:2]).T).T\n",
    "        if np.all(np.fabs(temp_obstacle_local_coord) < inst_ctr_size/2):\n",
    "            selected_obstacle_tokens.append(temp_obstacle_token)\n",
    "    for temp_obstacle_token in selected_obstacle_tokens:\n",
    "        temp_obstacle = ds.get('obstacle', temp_obstacle_token)\n",
    "        # temp_obstacle_local_coord = np.dot(np.array(temp_obstacle['coords']) - curr_pose[:2], rot)\n",
    "        temp_obstacle_local_coord = np.dot(rot, (np.array(temp_obstacle['coords']) - curr_pose[:2]).T).T\n",
    "        temp_obstacle_local_heading = temp_obstacle['heading'] - curr_pose[2]\n",
    "        temp_obstacle_local_pose = np.concatenate([temp_obstacle_local_coord, [temp_obstacle_local_heading]])\n",
    "        temp_obstacle_4_vertices = get_box_pts_from_center_heading(length=temp_obstacle['size'][0], width=temp_obstacle['size'][1], xc=temp_obstacle_local_pose[0], yc=temp_obstacle_local_pose[1], heading=temp_obstacle_local_pose[2] / pi * 180)\n",
    "        temp_obstacle_4_vertices = np.array(temp_obstacle_4_vertices)\n",
    "        veh.obstacle_4_vertices_list.append(temp_obstacle_4_vertices)\n",
    "    \n",
    "    # 获取局部地图窗口内的路点\n",
    "    all_waypoints = np.empty((0, 2))\n",
    "    for _, points in vis.waypoints.items():\n",
    "        all_waypoints = np.append(all_waypoints, points, axis=0)\n",
    "    # all_waypoints_local_coord = np.dot(all_waypoints - curr_pose[:2], rot)\n",
    "    all_waypoints_local_coord = np.dot(rot, (all_waypoints - curr_pose[:2]).T).T\n",
    "    selected_waypoints_idx = np.where(np.all(np.fabs(all_waypoints_local_coord) < inst_ctr_size/2, axis=1))[0]\n",
    "    veh.waypoints_local_coord = all_waypoints_local_coord[selected_waypoints_idx]\n",
    "\n",
    "    # 获取局部地图窗口内的停车位信息\n",
    "    all_parking_4_vertices = vis.parking_spaces.iloc[:, -8:].values.reshape(-1, 2)\n",
    "    all_parking_4_vertices_local_coord = np.dot(rot, (all_parking_4_vertices - curr_pose[:2]).T).T.reshape(-1, 4, 2)\n",
    "    selected_parking_idx = np.where( np.all(np.all(np.fabs(all_parking_4_vertices_local_coord) < inst_ctr_size/2, axis=2), axis=1) )[0]\n",
    "    veh.parkings_4_vertices_local_coord = all_parking_4_vertices_local_coord[selected_parking_idx]\n",
    "\n",
    "    # AV目标点\n",
    "    img_frame = extractor.vis.plot_frame(frame_token)\n",
    "    image_feature = extractor.vis.inst_centric(img_frame, inst_token)\n",
    "    global_intent_pose = extractor.get_intent_pose(inst_token=inst_token, inst_centric_view=image_feature)\n",
    "    local_intent_coords = np.dot(rot, global_intent_pose[:2]-curr_pose[:2])\n",
    "    local_intent_pose = np.array(\n",
    "        [local_intent_coords[0], local_intent_coords[1], global_intent_pose[2]-curr_pose[2]])\n",
    "    veh.av_intent_pose = local_intent_pose    \n",
    "    # 保存veh为pickle文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1121 [00:31<53:23,  2.89s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# with multiprocessing.Pool(processes=cpu_count) as pool:\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#     inputs = zip(all_instance_tokens, all_instance_indices, [frame_token]*num_insts, [extractor]*num_insts, [ds]*num_insts, [agent_token2idx]*num_insts, [vis]*num_insts)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#     results = pool.starmap(get_data_for_instance, inputs)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inst_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_insts):\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mget_data_for_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_instance_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_instance_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_token2idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# get_data_for_instance(all_instance_tokens[0], all_instance_indices[0], frame_token, extractor, ds, agent_token2idx, vis)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 109\u001b[0m, in \u001b[0;36mget_data_for_instance\u001b[0;34m(inst_token, inst_idx, frame_token, extractor, ds, agent_token2idx, vis)\u001b[0m\n\u001b[1;32m    107\u001b[0m img_frame \u001b[38;5;241m=\u001b[39m extractor\u001b[38;5;241m.\u001b[39mvis\u001b[38;5;241m.\u001b[39mplot_frame(frame_token)\n\u001b[1;32m    108\u001b[0m image_feature \u001b[38;5;241m=\u001b[39m extractor\u001b[38;5;241m.\u001b[39mvis\u001b[38;5;241m.\u001b[39minst_centric(img_frame, inst_token)\n\u001b[0;32m--> 109\u001b[0m global_intent_pose \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_intent_pose\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minst_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst_centric_view\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_feature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m local_intent_coords \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(rot, global_intent_pose[:\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m-\u001b[39mcurr_pose[:\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m    111\u001b[0m local_intent_pose \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m    112\u001b[0m     [local_intent_coords[\u001b[38;5;241m0\u001b[39m], local_intent_coords[\u001b[38;5;241m1\u001b[39m], global_intent_pose[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m-\u001b[39mcurr_pose[\u001b[38;5;241m2\u001b[39m]])\n",
      "File \u001b[0;32m~/ParkSim/python/parksim/trajectory_predict/data_processing/utils.py:99\u001b[0m, in \u001b[0;36mTransformerDataProcessor.get_intent_pose\u001b[0;34m(self, inst_token, inst_centric_view, center_pose, r)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03mreturns global pose (x, y, heading) coordinates of the intent. \u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03mIf the future traj goes inside a spot, use the spot center + vehicle pose as result. If the traj goes outside of the view, use the last visible state as result\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m all_spots \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspot_detector\u001b[38;5;241m.\u001b[39mdetect(inst_centric_view)\n\u001b[0;32m---> 99\u001b[0m traj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_future_traj\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstance\u001b[39m\u001b[38;5;124m'\u001b[39m, inst_token)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m center_pose \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/dlp-dataset/dlp/dataset.py:196\u001b[0m, in \u001b[0;36mDataset.get_future_traj\u001b[0;34m(self, inst_token, static_thres)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m next_token:\n\u001b[1;32m    195\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstance\u001b[39m\u001b[38;5;124m'\u001b[39m, next_token)\n\u001b[0;32m--> 196\u001b[0m     signed_speed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigned_speed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     traj\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray([instance[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoords\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m], instance[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoords\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m], instance[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheading\u001b[39m\u001b[38;5;124m'\u001b[39m], signed_speed]))\n\u001b[1;32m    199\u001b[0m     next_token \u001b[38;5;241m=\u001b[39m instance[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/dlp-dataset/dlp/dataset.py:175\u001b[0m, in \u001b[0;36mDataset.signed_speed\u001b[0;34m(self, inst_token)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m     prev_inst \u001b[38;5;241m=\u001b[39m instance\n\u001b[0;32m--> 175\u001b[0m motion_vector \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m(next_inst[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoords\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39marray(prev_inst[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoords\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m heading_vector \u001b[38;5;241m@\u001b[39m motion_vector \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m instance[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeed\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "extractor = TransformerDataProcessor(ds=ds, tail_size=10)\n",
    "all_frames = []\n",
    "frame_token = scene['first_frame']\n",
    "while frame_token:\n",
    "    all_frames.append(frame_token)\n",
    "    frame = ds.get('frame', frame_token)\n",
    "    frame_token = frame['next']\n",
    "for frame_idx in tqdm(range(stride*history, len(all_frames) - stride*future, stride)):\n",
    "    frame_token = all_frames[frame_idx]\n",
    "    all_instance_tokens, all_instance_indices = extractor.filter_instances(frame_token, stride, history, future)\n",
    "    num_insts = len(all_instance_tokens)\n",
    "    # with multiprocessing.Pool(processes=cpu_count) as pool:\n",
    "    #     inputs = zip(all_instance_tokens, all_instance_indices, [frame_token]*num_insts, [extractor]*num_insts, [ds]*num_insts, [agent_token2idx]*num_insts, [vis]*num_insts)\n",
    "    #     results = pool.starmap(get_data_for_instance, inputs)\n",
    "    for inst_idx in range(num_insts):\n",
    "        get_data_for_instance(all_instance_tokens[inst_idx], all_instance_indices[inst_idx], frame_token, extractor, ds, agent_token2idx, vis)\n",
    "    # get_data_for_instance(all_instance_tokens[0], all_instance_indices[0], frame_token, extractor, ds, agent_token2idx, vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for timestep in range(stride, stride*history+1, stride):\n",
    "    print(timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([np.array([1,2,3]), ([4])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NNDE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
