{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "import os\n",
    "\n",
    "from dlp.dataset import Dataset\n",
    "from parksim.trajectory_predict.intent_transformer.network import TrajectoryPredictorWithIntentV2\n",
    "from parksim.trajectory_predict.data_processing.utils import TransformerDataProcessor\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trajectory_predict_from_config(config, input_shape=(3, 100, 100)):\n",
    "    model = TrajectoryPredictorWithIntentV2(\n",
    "        input_shape=input_shape,\n",
    "        dropout=config['dropout'], \n",
    "        num_heads=config['num_heads'], \n",
    "        num_encoder_layers=config['num_encoder_layers'], \n",
    "        num_decoder_layers=config['num_decoder_layers'], \n",
    "        dim_model=config['dim_model'],\n",
    "        d_hidden=config['d_hidden'],\n",
    "        num_conv_layers=config['num_conv_layers']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"models\\Intent_Transformer_v2_04-13-2022_11-10-07.pth\"\n",
    "config={\n",
    "            'dim_model' : 64,\n",
    "            'num_heads' : 8,\n",
    "            'dropout' : 0.0,\n",
    "            'num_encoder_layers' : 4,\n",
    "            'num_decoder_layers' : 4,\n",
    "            'd_hidden' : 256,\n",
    "            'num_conv_layers' : 2,\n",
    "            'opt' : 'Adam',\n",
    "            'lr' : 5e-5,\n",
    "            'loss' : 'L1'\n",
    "    }\n",
    "\n",
    "model = build_trajectory_predict_from_config(config)\n",
    "model_state = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval().to(DEVICE)\n",
    "\n",
    "img_transform=transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dji_num = '0012'\n",
    "\n",
    "home_path = Path.home() / 'Documents/GitHub'\n",
    "# Load dataset\n",
    "ds = Dataset()\n",
    "ds.load(str(home_path / f'dlp-dataset/data/DJI_{dji_num}'))\n",
    "\n",
    "extractor = TransformerDataProcessor(ds=ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = ds.get('scene', ds.list_scenes()[0])\n",
    "agents = scene['agents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_for_agent_in_range(agent_token: str, extractor: TransformerDataProcessor, instances, start_idx: int, stride: int=10, history: int=10, future: int=10, img_size: int=100):\n",
    "    all_image_history = []\n",
    "    all_trajectory_history = []\n",
    "    all_trajectory_future = []\n",
    "    all_trajectory_future_tgt = []\n",
    "    all_local_intent_pose = []\n",
    "    all_inst_centric_view = []\n",
    "    inst_idx = start_idx\n",
    "    curr_instance = instances[inst_idx]\n",
    "    inst_token = curr_instance['instance_token']\n",
    "    img_frame = extractor.vis.plot_frame(curr_instance['frame_token'])\n",
    "    image_feature = extractor.vis.inst_centric(img_frame, inst_token)\n",
    "\n",
    "    global_intent_pose = extractor.get_intent_pose(\n",
    "        inst_token=inst_token, inst_centric_view=image_feature)\n",
    "\n",
    "    image_feature = extractor.label_target_spot(inst_token, image_feature)\n",
    "\n",
    "    all_inst_centric_view = image_feature.copy()\n",
    "\n",
    "    curr_pose = np.array([curr_instance['coords'][0],\n",
    "                            curr_instance['coords'][1], curr_instance['heading']])\n",
    "    rot = np.array([[np.cos(-curr_pose[2]), -np.sin(-curr_pose[2])],\n",
    "                [np.sin(-curr_pose[2]), np.cos(-curr_pose[2])]])\n",
    "\n",
    "    local_intent_coords = np.dot(rot, global_intent_pose[:2]-curr_pose[:2])\n",
    "    local_intent_pose = np.array(\n",
    "        [local_intent_coords[0], local_intent_coords[1], global_intent_pose[2]-curr_pose[2]])\n",
    "    local_intent_pose = np.expand_dims(local_intent_pose, axis=0)\n",
    "\n",
    "    image_history = []\n",
    "    trajectory_history = []\n",
    "    for i in range(inst_idx - stride * (history - 1), inst_idx+1, stride):\n",
    "        instance = instances[i]\n",
    "        pos = np.array(instance['coords'])\n",
    "        translated_pos = np.dot(rot, pos-curr_pose[:2])\n",
    "        trajectory_history.append(Tensor(\n",
    "            [translated_pos[0], translated_pos[1], instance['heading'] - curr_pose[2]]))\n",
    "\n",
    "        # generate image history\n",
    "        img_frame = extractor.vis.plot_frame(instance['frame_token'])\n",
    "        image_feature = extractor.vis.inst_centric(\n",
    "            img_frame, instance['instance_token'], curr_pose)\n",
    "        image_feature = extractor.label_target_spot(\n",
    "            inst_token, image_feature, curr_pose)\n",
    "\n",
    "        # Image transformation\n",
    "        image_tensor = img_transform(image_feature.resize((img_size, img_size)))\n",
    "        image_history.append(image_tensor)\n",
    "    \n",
    "    trajectory_future = []\n",
    "    for i in range(inst_idx + stride, inst_idx + stride * future + 1, stride):\n",
    "        instance = instances[i]\n",
    "        pos = np.array(instance['coords'])\n",
    "        translated_pos = np.dot(rot, pos-curr_pose[:2])\n",
    "        trajectory_future.append(Tensor(\n",
    "            [translated_pos[0], translated_pos[1], instance['heading'] - curr_pose[2]]))\n",
    "    \n",
    "    all_image_history = torch.stack(image_history)\n",
    "    all_trajectory_history = torch.stack(trajectory_history)\n",
    "    # This is the tgt that is passed into the decoder, and trajectory_future is the label\n",
    "    trajectory_future_tgt = torch.stack(\n",
    "        trajectory_history[-1:] + trajectory_future[:-1])\n",
    "    all_trajectory_future_tgt = trajectory_future_tgt\n",
    "    all_trajectory_future = torch.stack(trajectory_future)\n",
    "    all_local_intent_pose = torch.from_numpy(local_intent_pose)\n",
    "\n",
    "    return all_image_history, all_trajectory_history, all_local_intent_pose, all_trajectory_future_tgt, all_trajectory_future, all_inst_centric_view\n",
    "\n",
    "def generate_data_for_agent(agent_token: str, extractor: TransformerDataProcessor, stride: int=10, history: int=10, future: int=10, img_size: int=100):\n",
    "    instances = ds.get_agent_instances(agent_token)\n",
    "    start_idx = history * stride\n",
    "    end_idx = (len(instances) - 1 - future * stride) // 100\n",
    "    all_inputs = [(agent_token, extractor, instances, start_idx + i * future * stride, stride, history, future, img_size) for i in range(end_idx - start_idx)]\n",
    "    print(len(all_inputs))\n",
    "    with multiprocessing.Pool(processes=32) as pool:\n",
    "        result = pool.starmap(generate_data_for_agent_in_range, all_inputs)\n",
    "        print(\"HERE\")\n",
    "    get_ith_index_list = lambda i : lambda t : t[i]\n",
    "    return torch.stack(map(get_ith_index_list(0), result)), torch.stack(map(get_ith_index_list(1), result)), torch.stack(map(get_ith_index_list(2), result)), torch.stack(map(get_ith_index_list(3), result)), torch.stack(map(get_ith_index_list(4), result)), map(get_ith_index_list(5), result)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_prediction(idx):\n",
    "    sensing_limit = 20\n",
    "\n",
    "    inst_centric_view = list_inst_centric_view[idx]\n",
    "\n",
    "    img_size = inst_centric_view.size[0] / 2\n",
    "\n",
    "    traj_hist_pixel = X[idx, :, :2].detach().cpu().numpy() / \\\n",
    "        sensing_limit*img_size + img_size\n",
    "\n",
    "    traj_future_pixel = y_label[idx, :, :2].detach().cpu().numpy() / \\\n",
    "        sensing_limit*img_size + img_size\n",
    "\n",
    "    intent_pixel = intent[idx, 0, :2].detach().cpu().numpy() / \\\n",
    "        sensing_limit*img_size + img_size\n",
    "\n",
    "    traj_pred_pixel = pred[idx, :, :2].detach().cpu().numpy() / \\\n",
    "        sensing_limit*img_size + img_size\n",
    "\n",
    "    plt.cla()\n",
    "\n",
    "    plt.imshow(inst_centric_view)\n",
    "    plt.plot(traj_hist_pixel[:, 0], traj_hist_pixel[:, 1], 'k', linewidth=2)\n",
    "    plt.plot(traj_future_pixel[:, 0], traj_future_pixel[:,\n",
    "            1], 'wo', linewidth=2, markersize=2)\n",
    "    plt.plot(traj_pred_pixel[:, 0], traj_pred_pixel[:, 1],\n",
    "            'g^', linewidth=2, markersize=2)\n",
    "    plt.plot(intent_pixel[0], intent_pixel[1], '*', color='C1', markersize=8)\n",
    "    plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Car\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "agent_token = agents[6]\n",
    "print(ds.get('agent', agent_token)['type'])\n",
    "img, X, intent, y_in, y_label, list_inst_centric_view = generate_data_for_agent(agent_token=agent_token, extractor=extractor)\n",
    "with torch.no_grad():\n",
    "    img = img.to(DEVICE).float()\n",
    "    X = X.to(DEVICE).float()\n",
    "    intent = intent.to(DEVICE).float()\n",
    "    y_in = y_in.to(DEVICE).float()\n",
    "    y_label = y_label.to(DEVICE).float()\n",
    "    tgt_mask = model.transformer.generate_square_subsequent_mask(\n",
    "        y_in.shape[1]).to(DEVICE).float()\n",
    "\n",
    "    pred = model(img, X, intent, y_in, tgt_mask=tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rlaca\\Documents\\GitHub\\ParkSim\\python\\parksim\\trajectory_predict\\intent_transformer\\visualize.ipynb Cell 13'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rlaca/Documents/GitHub/ParkSim/python/parksim/trajectory_predict/intent_transformer/visualize.ipynb#ch0000012?line=0'>1</a>\u001b[0m frame_i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rlaca/Documents/GitHub/ParkSim/python/parksim/trajectory_predict/intent_transformer/visualize.ipynb#ch0000012?line=1'>2</a>\u001b[0m fig \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mfigure()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rlaca/Documents/GitHub/ParkSim/python/parksim/trajectory_predict/intent_transformer/visualize.ipynb#ch0000012?line=3'>4</a>\u001b[0m draw_prediction(frame_i)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rlaca/Documents/GitHub/ParkSim/python/parksim/trajectory_predict/intent_transformer/visualize.ipynb#ch0000012?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "frame_i = 0\n",
    "fig = plt.figure()\n",
    "\n",
    "draw_prediction(frame_i)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_inst_centric_view' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rlaca\\Documents\\GitHub\\ParkSim\\python\\parksim\\trajectory_predict\\intent_transformer\\visualize.ipynb Cell 15'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rlaca/Documents/GitHub/ParkSim/python/parksim/trajectory_predict/intent_transformer/visualize.ipynb#ch0000014?line=4'>5</a>\u001b[0m fname \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mUsers\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mrlaca\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mDocuments\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mGitHub\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mParkSim\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mpython\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mparksim\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtrajectory_predict\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mintent_transformer\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39manimation.mp4\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rlaca/Documents/GitHub/ParkSim/python/parksim/trajectory_predict/intent_transformer/visualize.ipynb#ch0000014?line=5'>6</a>\u001b[0m video_writer \u001b[39m=\u001b[39m animation\u001b[39m.\u001b[39mFFMpegWriter(fps\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rlaca/Documents/GitHub/ParkSim/python/parksim/trajectory_predict/intent_transformer/visualize.ipynb#ch0000014?line=6'>7</a>\u001b[0m anim\u001b[39m.\u001b[39;49msave(fname, writer\u001b[39m=\u001b[39;49mvideo_writer)\n",
      "File \u001b[1;32m~\\Envs\\parksim\\lib\\site-packages\\matplotlib\\animation.py:1160\u001b[0m, in \u001b[0;36mAnimation.save\u001b[1;34m(self, filename, writer, fps, dpi, codec, bitrate, extra_args, metadata, extra_anim, savefig_kwargs, progress_callback)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/rlaca/Envs/parksim/lib/site-packages/matplotlib/animation.py?line=1154'>1155</a>\u001b[0m \u001b[39mwith\u001b[39;00m mpl\u001b[39m.\u001b[39mrc_context({\u001b[39m'\u001b[39m\u001b[39msavefig.bbox\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mNone\u001b[39;00m}), \\\n\u001b[0;32m   <a href='file:///c%3A/Users/rlaca/Envs/parksim/lib/site-packages/matplotlib/animation.py?line=1155'>1156</a>\u001b[0m      writer\u001b[39m.\u001b[39msaving(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fig, filename, dpi), \\\n\u001b[0;32m   <a href='file:///c%3A/Users/rlaca/Envs/parksim/lib/site-packages/matplotlib/animation.py?line=1156'>1157</a>\u001b[0m      cbook\u001b[39m.\u001b[39m_setattr_cm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fig\u001b[39m.\u001b[39mcanvas,\n\u001b[0;32m   <a href='file:///c%3A/Users/rlaca/Envs/parksim/lib/site-packages/matplotlib/animation.py?line=1157'>1158</a>\u001b[0m                        _is_saving\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, manager\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   <a href='file:///c%3A/Users/rlaca/Envs/parksim/lib/site-packages/matplotlib/animation.py?line=1158'>1159</a>\u001b[0m     \u001b[39mfor\u001b[39;00m anim \u001b[39min\u001b[39;00m all_anim:\n\u001b[1;32m-> <a href='file:///c%3A/Users/rlaca/Envs/parksim/lib/site-packages/matplotlib/animation.py?line=1159'>1160</a>\u001b[0m         anim\u001b[39m.\u001b[39;49m_init_draw()  \u001b[39m# Clear the initial frame\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/rlaca/Envs/parksim/lib/site-packages/matplotlib/animation.py?line=1160'>1161</a>\u001b[0m     frame_number \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/rlaca/Envs/parksim/lib/site-packages/matplotlib/animation.py?line=1161'>1162</a>\u001b[0m     \u001b[39m# TODO: Currently only FuncAnimation has a save_count\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/rlaca/Envs/parksim/lib/site-packages/matplotlib/animation.py?line=1162'>1163</a>\u001b[0m     \u001b[39m#       attribute. Can we generalize this to all Animations?\u001b[39;00m\n",
      "File \u001b[1;32m~\\Envs\\parksim\\lib\\site-packages\\matplotlib\\animation.py:1753\u001b[0m, in \u001b[0;36mFuncAnimation._init_draw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/rlaca/Envs/parksim/lib/site-packages/matplotlib/animation.py?line=1747'>1748</a>\u001b[0m \u001b[39m# Initialize the drawing either using the given init_func or by\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/rlaca/Envs/parksim/lib/site-packages/matplotlib/animation.py?line=1748'>1749</a>\u001b[0m \u001b[39m# calling the draw function with the first item of the frame sequence.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/rlaca/Envs/parksim/lib/site-packages/matplotlib/animation.py?line=1749'>1750</a>\u001b[0m \u001b[39m# For blitting, the init_func should return a sequence of modified\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/rlaca/Envs/parksim/lib/site-packages/matplotlib/animation.py?line=1750'>1751</a>\u001b[0m \u001b[39m# artists.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/rlaca/Envs/parksim/lib/site-packages/matplotlib/animation.py?line=1751'>1752</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_func \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/rlaca/Envs/parksim/lib/site-packages/matplotlib/animation.py?line=1752'>1753</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_draw_frame(\u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnew_frame_seq()))\n\u001b[0;32m   <a href='file:///c%3A/Users/rlaca/Envs/parksim/lib/site-packages/matplotlib/animation.py?line=1754'>1755</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/rlaca/Envs/parksim/lib/site-packages/matplotlib/animation.py?line=1755'>1756</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_drawn_artists \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_func()\n",
      "File \u001b[1;32m~\\Envs\\parksim\\lib\\site-packages\\matplotlib\\animation.py:1776\u001b[0m, in \u001b[0;36mFuncAnimation._draw_frame\u001b[1;34m(self, framedata)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/rlaca/Envs/parksim/lib/site-packages/matplotlib/animation.py?line=1771'>1772</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_seq \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_seq[\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_count:]\n\u001b[0;32m   <a href='file:///c%3A/Users/rlaca/Envs/parksim/lib/site-packages/matplotlib/animation.py?line=1773'>1774</a>\u001b[0m \u001b[39m# Call the func with framedata and args. If blitting is desired,\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/rlaca/Envs/parksim/lib/site-packages/matplotlib/animation.py?line=1774'>1775</a>\u001b[0m \u001b[39m# func needs to return a sequence of any artists that were modified.\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/rlaca/Envs/parksim/lib/site-packages/matplotlib/animation.py?line=1775'>1776</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_drawn_artists \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(framedata, \u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_args)\n\u001b[0;32m   <a href='file:///c%3A/Users/rlaca/Envs/parksim/lib/site-packages/matplotlib/animation.py?line=1777'>1778</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blit:\n\u001b[0;32m   <a href='file:///c%3A/Users/rlaca/Envs/parksim/lib/site-packages/matplotlib/animation.py?line=1779'>1780</a>\u001b[0m     err \u001b[39m=\u001b[39m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mThe animation function must return a sequence \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/rlaca/Envs/parksim/lib/site-packages/matplotlib/animation.py?line=1780'>1781</a>\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mof Artist objects.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\rlaca\\Documents\\GitHub\\ParkSim\\python\\parksim\\trajectory_predict\\intent_transformer\\visualize.ipynb Cell 8'\u001b[0m in \u001b[0;36mdraw_prediction\u001b[1;34m(idx)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rlaca/Documents/GitHub/ParkSim/python/parksim/trajectory_predict/intent_transformer/visualize.ipynb#ch0000007?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw_prediction\u001b[39m(idx):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rlaca/Documents/GitHub/ParkSim/python/parksim/trajectory_predict/intent_transformer/visualize.ipynb#ch0000007?line=1'>2</a>\u001b[0m     sensing_limit \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rlaca/Documents/GitHub/ParkSim/python/parksim/trajectory_predict/intent_transformer/visualize.ipynb#ch0000007?line=3'>4</a>\u001b[0m     inst_centric_view \u001b[39m=\u001b[39m list_inst_centric_view[idx]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rlaca/Documents/GitHub/ParkSim/python/parksim/trajectory_predict/intent_transformer/visualize.ipynb#ch0000007?line=5'>6</a>\u001b[0m     img_size \u001b[39m=\u001b[39m inst_centric_view\u001b[39m.\u001b[39msize[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rlaca/Documents/GitHub/ParkSim/python/parksim/trajectory_predict/intent_transformer/visualize.ipynb#ch0000007?line=7'>8</a>\u001b[0m     traj_hist_pixel \u001b[39m=\u001b[39m X[idx, :, :\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy() \u001b[39m/\u001b[39m \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rlaca/Documents/GitHub/ParkSim/python/parksim/trajectory_predict/intent_transformer/visualize.ipynb#ch0000007?line=8'>9</a>\u001b[0m         sensing_limit\u001b[39m*\u001b[39mimg_size \u001b[39m+\u001b[39m img_size\n",
      "\u001b[1;31mNameError\u001b[0m: name 'list_inst_centric_view' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 750x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "anim = animation.FuncAnimation(fig, draw_prediction, frames=pred.shape[0],\n",
    "                               interval=0.4)\n",
    "fname = 'C:\\\\Users\\\\rlaca\\\\Documents\\\\GitHub\\\\ParkSim\\\\python\\\\parksim\\\\trajectory_predict\\\\intent_transformer\\\\animation.mp4'\n",
    "video_writer = animation.FFMpegWriter(fps=2)\n",
    "anim.save(fname, writer=video_writer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Animations for all agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./animations'):\n",
    "    os.mkdir('./animations')\n",
    "\n",
    "for i, agent_token in enumerate(agents):\n",
    "    if ds.get('agent', agent_token)['type'] in {'Pedestrian', 'Undefined'}:\n",
    "        continue\n",
    "    \n",
    "    print(f'===== Generating animation for agent {i} / {len(agents)} ======')\n",
    "\n",
    "    try:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img, X, intent, y_in, y_label, list_inst_centric_view = generate_data_for_agent(\n",
    "            agent_token=agent_token, extractor=extractor)\n",
    "            \n",
    "            img = img.to(DEVICE).float()\n",
    "            X = X.to(DEVICE).float()\n",
    "            intent = intent.to(DEVICE).float()\n",
    "            y_in = y_in.to(DEVICE).float()\n",
    "            y_label = y_label.to(DEVICE).float()\n",
    "            tgt_mask = model.transformer.generate_square_subsequent_mask(\n",
    "                y_in.shape[1]).to(DEVICE).float()\n",
    "\n",
    "            pred = model(img, X, intent, y_in, tgt_mask=tgt_mask)\n",
    "\n",
    "        fig = plt.figure()\n",
    "\n",
    "        anim = animation.FuncAnimation(fig, draw_prediction, frames=pred.shape[0],\n",
    "                                    interval=0.1)\n",
    "\n",
    "        fname = f'./animations/DJI_{dji_num}-Agent_{i}.mp4'\n",
    "        video_writer = animation.FFMpegWriter(fps=10)\n",
    "        anim.save(fname, writer=video_writer)\n",
    "\n",
    "    except:\n",
    "        print(f'Get error at agent {i}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "427b3bd9b9dd19d37481672008534dffed4e74dc6f302a9b19947aa559295010"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('park-sim')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
