{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "from dlp.dataset import Dataset\n",
    "from parksim.trajectory_predict.intent_transformer.network import TrajectoryPredictorWithIntent\n",
    "from parksim.trajectory_predict.data_processing.utils import TransformerDataProcessor\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"models/Intent_Transformer_all_data_03-19-2022_20-19-01.pth\"\n",
    "\n",
    "model = TrajectoryPredictorWithIntent()\n",
    "model_state = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval().to(DEVICE)\n",
    "\n",
    "img_transform=transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dji_num = '0012'\n",
    "\n",
    "home_path = str(Path.home())\n",
    "# Load dataset\n",
    "ds = Dataset()\n",
    "ds.load(home_path + f'/dlp-dataset/data/DJI_{dji_num}')\n",
    "\n",
    "extractor = TransformerDataProcessor(ds=ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Certain Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_for_agent(agent_token: str, extractor: TransformerDataProcessor, stride: int=10, history: int=10, future: int=10, img_size: int=100):\n",
    "    instances = ds.get_agent_instances(agent_token)\n",
    "    start_idx = history * stride\n",
    "    end_idx = len(instances) - 1 - future*stride\n",
    "\n",
    "    all_image_history = []\n",
    "    all_trajectory_history = []\n",
    "    all_trajectory_future = []\n",
    "    all_trajectory_future_tgt = []\n",
    "    all_local_intent_pose = []\n",
    "    all_inst_centric_view = []\n",
    "\n",
    "    for inst_idx in tqdm(range(start_idx, end_idx, stride)):\n",
    "        curr_instance = instances[inst_idx]\n",
    "        inst_token = curr_instance['instance_token']\n",
    "\n",
    "        img_frame = extractor.vis.plot_frame(curr_instance['frame_token'])\n",
    "        image_feature = extractor.vis.inst_centric(img_frame, inst_token)\n",
    "\n",
    "        global_intent_pose = extractor.get_intent_pose(\n",
    "            inst_token=inst_token, inst_centric_view=image_feature)\n",
    "\n",
    "        image_feature = extractor.label_target_spot(inst_token, image_feature)\n",
    "\n",
    "        all_inst_centric_view.append(image_feature.copy())\n",
    "\n",
    "        curr_pose = np.array([curr_instance['coords'][0],\n",
    "                              curr_instance['coords'][1], curr_instance['heading']])\n",
    "        rot = np.array([[np.cos(-curr_pose[2]), -np.sin(-curr_pose[2])],\n",
    "                   [np.sin(-curr_pose[2]), np.cos(-curr_pose[2])]])\n",
    "\n",
    "        local_intent_coords = np.dot(rot, global_intent_pose[:2]-curr_pose[:2])\n",
    "        local_intent_pose = np.array(\n",
    "            [local_intent_coords[0], local_intent_coords[1], global_intent_pose[2]-curr_pose[2]])\n",
    "        local_intent_pose = np.expand_dims(local_intent_pose, axis=0)\n",
    "\n",
    "        image_history = []\n",
    "        trajectory_history = []\n",
    "        for i in range(inst_idx - stride * (history - 1), inst_idx+1, stride):\n",
    "            instance = instances[i]\n",
    "            pos = np.array(instance['coords'])\n",
    "            translated_pos = np.dot(rot, pos-curr_pose[:2])\n",
    "            trajectory_history.append(Tensor(\n",
    "                [translated_pos[0], translated_pos[1], instance['heading'] - curr_pose[2]]))\n",
    "\n",
    "            # generate image history\n",
    "            img_frame = extractor.vis.plot_frame(instance['frame_token'])\n",
    "            image_feature = extractor.vis.inst_centric(\n",
    "                img_frame, instance['instance_token'], curr_pose)\n",
    "            image_feature = extractor.label_target_spot(\n",
    "                inst_token, image_feature, curr_pose)\n",
    "\n",
    "            # Image transformation\n",
    "            image_tensor = img_transform(image_feature.resize((img_size, img_size)))\n",
    "            image_history.append(image_tensor)\n",
    "        \n",
    "        trajectory_future = []\n",
    "        for i in range(inst_idx + stride, inst_idx + stride * future + 1, stride):\n",
    "            instance = instances[i]\n",
    "            pos = np.array(instance['coords'])\n",
    "            translated_pos = np.dot(rot, pos-curr_pose[:2])\n",
    "            trajectory_future.append(Tensor(\n",
    "                [translated_pos[0], translated_pos[1], instance['heading'] - curr_pose[2]]))\n",
    "        \n",
    "        all_image_history.append(torch.stack(image_history))\n",
    "        all_trajectory_history.append(torch.stack(trajectory_history))\n",
    "        # This is the tgt that is passed into the decoder, and trajectory_future is the label\n",
    "        trajectory_future_tgt = torch.stack(\n",
    "            trajectory_history[-1:] + trajectory_future[:-1])\n",
    "        all_trajectory_future_tgt.append(trajectory_future_tgt)\n",
    "        all_trajectory_future.append(torch.stack(trajectory_future))\n",
    "        all_local_intent_pose.append(torch.from_numpy(local_intent_pose))\n",
    "\n",
    "    return torch.stack(all_image_history), torch.stack(all_trajectory_history), torch.stack(all_local_intent_pose), torch.stack(all_trajectory_future_tgt), torch.stack(all_trajectory_future), all_inst_centric_view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = ds.get('scene', ds.list_scenes()[0])\n",
    "agents = scene['agents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_token = agents[4]\n",
    "print(ds.get('agent', agent_token)['type'])\n",
    "img, X, intent, y_in, y_label, list_inst_centric_view = generate_data_for_agent(agent_token=agent_token, extractor=extractor)\n",
    "\n",
    "img = img.to(DEVICE).float()\n",
    "X = X.to(DEVICE).float()\n",
    "intent = intent.to(DEVICE).float()\n",
    "y_in = y_in.to(DEVICE).float()\n",
    "y_label = y_label.to(DEVICE).float()\n",
    "tgt_mask = model.transformer.generate_square_subsequent_mask(\n",
    "    y_in.shape[1]).to(DEVICE).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(img, X, intent, y_in, tgt_mask=tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_prediction(idx):\n",
    "    sensing_limit = 20\n",
    "\n",
    "    inst_centric_view = list_inst_centric_view[idx]\n",
    "\n",
    "    img_size = inst_centric_view.size[0] / 2\n",
    "\n",
    "    traj_hist_pixel = X[idx, :, :2].detach().numpy() / \\\n",
    "        sensing_limit*img_size + img_size\n",
    "\n",
    "    traj_future_pixel = y_label[idx, :, :2].detach().numpy() / \\\n",
    "        sensing_limit*img_size + img_size\n",
    "\n",
    "    intent_pixel = intent[idx, 0, :2].detach().numpy() / \\\n",
    "        sensing_limit*img_size + img_size\n",
    "\n",
    "    traj_pred_pixel = pred[idx, :, :2].detach().numpy() / \\\n",
    "        sensing_limit*img_size + img_size\n",
    "\n",
    "    plt.cla()\n",
    "\n",
    "    plt.imshow(inst_centric_view)\n",
    "    plt.plot(traj_hist_pixel[:, 0], traj_hist_pixel[:, 1], 'k', linewidth=2)\n",
    "    plt.plot(traj_future_pixel[:, 0], traj_future_pixel[:,\n",
    "            1], 'wo', linewidth=2, markersize=2)\n",
    "    plt.plot(traj_pred_pixel[:, 0], traj_pred_pixel[:, 1],\n",
    "            'g^', linewidth=2, markersize=2)\n",
    "    plt.plot(intent_pixel[0], intent_pixel[1], '*', color='C1', markersize=8)\n",
    "    plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3\n",
    "fig = plt.figure()\n",
    "\n",
    "draw_prediction(idx)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "anim = animation.FuncAnimation(fig, draw_prediction, frames=pred.shape[0],\n",
    "                               interval=0.4)\n",
    "\n",
    "fname = 'animation.mp4'\n",
    "video_writer = animation.FFMpegWriter(fps=2)\n",
    "anim.save(fname, writer=video_writer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Animations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./animations'):\n",
    "    os.mkdir('./animations')\n",
    "\n",
    "for i, agent_token in enumerate(agents):\n",
    "    if ds.get('agent', agent_token)['type'] in {'Pedestrian', 'Undefined'}:\n",
    "        continue\n",
    "    \n",
    "    print(f'===== Generating animation for agent {i} / {len(agents)} ======')\n",
    "    img, X, intent, y_in, y_label, list_inst_centric_view = generate_data_for_agent(\n",
    "        agent_token=agent_token, extractor=extractor)\n",
    "\n",
    "    img = img.to(DEVICE).float()\n",
    "    X = X.to(DEVICE).float()\n",
    "    intent = intent.to(DEVICE).float()\n",
    "    y_in = y_in.to(DEVICE).float()\n",
    "    y_label = y_label.to(DEVICE).float()\n",
    "    tgt_mask = model.transformer.generate_square_subsequent_mask(\n",
    "        y_in.shape[1]).to(DEVICE).float()\n",
    "\n",
    "    pred = model(img, X, intent, y_in, tgt_mask=tgt_mask)\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, draw_prediction, frames=pred.shape[0],\n",
    "                                interval=0.4)\n",
    "\n",
    "    fname = f'./animations/DJI_{dji_num}-Agent_{i}.mp4'\n",
    "    video_writer = animation.FFMpegWriter(fps=2)\n",
    "    anim.save(fname, writer=video_writer)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "427b3bd9b9dd19d37481672008534dffed4e74dc6f302a9b19947aa559295010"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('park-sim')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
